---
layout: post
title: "Hadoop Eco System: Overview"
categories: hadoop
author: "Yongchan Hong"
---

# Hadoop Eco System Review
Hadoop이란 간단한 프로그래밍 모델로 빅데이터를 분산처리하게 해주는 시스템이다. Hadoop은 HDFS와 MapReduce로 시작되었으나 이제 Spark, Hive, HBase 등 많은 것들이 위에 올라가게 되었다. Hadoop은 Scale Out이 쉽기 때문에 현재 빅데이터 시대에 애용받고는 한다.   
![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fwevrm%2FbtqzEDKFAGI%2FrFI1UqX1JZ3uyCyEcfQ430%2Fimg.png)  
이러한 프레임워크의 대표적인 친구들인 HDFS, MapReduce, HBase, Hive에 대해 리뷰해 보겠다.

## HDFS
Hadoop Distributed File System (HDFS)은 블록 구조의 파일 저장 시스템이다. 하나의 Name Node를 가지고, 여러 개의 Data Node를 가지고 있다. 
![](https://wikidocs.net/images/page/23582/hdfsarchitecture.png)  
`Name Node`는 메타데이터를 관리하는데, 이는 네임스페이스와 블록정보, 트랜잭션 로그 등을 의미한다. 또한 데이터 노드를 관리하는데 데이터 노드가 보내는 하트비트와 블록 리포트를 사용한다.   

> 세컨더리 네임노드라는 예비용을 둔다. 

`Data Node`는 파일을 특정 크기의 블록으로 잘라서 저장한다. 보통 하나의 블록은 3개의 Data Node에 저장된다. 

> MapReduce를 이용할때 블록지역성 때문에 HDFS가 S3에 저장하는거 보다 빠르다. 

파일을 읽는건 FSDataInputStream을 통해 읽고, 쓰는건 FSDataOutputStream을 통해 쓴다.

## YARN
Yarn은 자원관리를 해주는 친구이다. 각 클러스터의 자원 (CPU, Memory)를 관리하고 작업을 스케줄링한다. MapReduce, Hive, Spark의 쿼리 요청이 올때 Resource Manager가 Node Manager와 통신하고 리소스 관리 및 작업 수행을 한다. 그러나 깊게는 들어가지않는다..(사실 필자는 써본적이 없고 아마 지속적으로 K8s에서 하지않을까? 싶다)

## MapReduce
MapReduce는 간단한 단위의 작업을 처리하는 Map과 이를 aggregate하는 MapReduce로 이루어져있다. 
![](https://i.stack.imgur.com/199Q1.png)  
MapReduce는 jar파일을 받아서 실행할 수 있다. 순서를 이야기하면...  
1. 먼저 input을 FileInputFormat을 이용하여 데이터를 읽는다.
2. 맵을 실행하고 <key, value> 형태로 반환한다.
3. Shuffling이 일어나며 맵의 결과들을 받아온다.
4. Reduce를 통해 합친다.
5. 값들을 모두 더해 출력한다.

## HBase
HDFS위에 만들어진 Column Family 기반 데이터베이스다. 읽기와 쓰기의 연관성이 있고, 사용자가 HDFS나 HBase에 Write할 수도 있다.  

## Hive
Hive는 데이터 웨어하우징 솔루션으로 Hadoop기반 데이터 분석을 가능하게 한다! (HadoopQL 가능)  
Hive의 가장 매력적인 부분은 Hive Metastore이다. 인용하면...
```
하이브의 메타정보는 파일의 물리적인 위치와 데이터에 대한 논리적인 정보로 구분할 수 있습니다. 이 메타정보를 보관하고 사용자의 요청에 따라 관련 정보를 제공하는 곳이 하이브 메타스토어입니다.

메타스토어는 쓰리프트 프로토콜을 이용하여 다른 서비스에 정보를 제공합니다. 메타 정보는 JDBC 드라이버를 이용하여 RDBMS에 저장됩니다.
```
이렇게 해서 MySQL같은 곳에 저장할 수 있다.

> 꿀팁: Spark SQL과 Hive Metastore를 쓰면 편하게 쿼리할 수 있다. 

## Avro
부호화 형식으로, 스키마와 타입을 지정할 수 있다. 읽기 스기마와 쓰키 스키마가 따로존재 한다. JSON으로 작성하며, 부호화시 압축이 잘되어 진다. 

## Parquet
칼럼 기반 저장 포맷으로 압축률이 좋으며 Hadoop Ecosystem에서 사용될 수 있다. 

### Reference
https://butter-shower.tistory.com/73  
https://wikidocs.net/23582 